{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWJ1rfWbiQ1S",
        "outputId": "88cb006a-faf1-4c86-9da2-db08d1fbfde0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s = 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Read Input\n",
        "list_input=[]\n",
        "for line in sys.stdin:\n",
        "    p, q = map(int, line.strip().split())\n",
        "    list_input.append((p, q))\n",
        "\n",
        "# Extract unique pages\n",
        "pages_set = set()\n",
        "page_to_index = {}\n",
        "for p, q in list_input:\n",
        "    pages_set.add(p)\n",
        "    pages_set.add(q)\n",
        "pages = sorted(list(pages_set))\n",
        "for index, page in enumerate(pages):\n",
        "    page_to_index[page] = index\n",
        "\n",
        "# Form adjacency list\n",
        "haslinksto = {page: [] for page in pages}\n",
        "haslinksfrom = {page: [] for page in pages}\n",
        "for p, q in list_input:\n",
        "    haslinksto[p].append(q)\n",
        "    haslinksfrom[q].append(p)\n",
        "\n",
        "# S=H+A\n",
        "m = len(pages)\n",
        "H_lite=[]\n",
        "\n",
        "for i in range(m):\n",
        "    temp = haslinksfrom[pages[i]]\n",
        "    temp1=[]\n",
        "    for k in temp:\n",
        "        index = page_to_index[k]\n",
        "        temp1.append([index, 1.0 / len(haslinksto[k])])\n",
        "    H_lite.append(temp1)\n",
        "\n",
        "\n",
        "A = np.zeros((1,m))\n",
        "for i in range(m):\n",
        "    if len(haslinksto[pages[i]]) == 0:\n",
        "      A[0][i]+=1.0/m\n",
        "\n",
        "I = np.ones((m, 1)) / m\n",
        "\n",
        "alpha = 0.85\n",
        "one = np.ones((1, m))\n",
        "\n",
        "\n",
        "# Calculating Google Matrix\n",
        "#G = alpha * H + alpha * A + (1 - alpha) * one / m\n",
        "\n",
        "# Initialize PageRank vector\n",
        "New = np.ones((m, 1)) / m\n",
        "\n",
        "# Power method till convergence\n",
        "iterations = 0\n",
        "max_iterations = 100\n",
        "tolerance = 1e-7\n",
        "while iterations < max_iterations:\n",
        "    New_next=np.zeros((m,1))\n",
        "\n",
        "    #HI\n",
        "    for i in range(m):\n",
        "      row_sum=0\n",
        "      for u in H_lite[i]:\n",
        "        row_sum+=New[u[0]]*u[1]\n",
        "      New_next[i]+=alpha*row_sum\n",
        "    #AI\n",
        "    row_sum=0\n",
        "    for i in range(m):\n",
        "      row_sum+=A[0][i]*New[i]\n",
        "    for i in range(m):\n",
        "      New_next[i]+=alpha*row_sum\n",
        "    #oneI\n",
        "    row_sum=0\n",
        "    for i in range(m):\n",
        "      row_sum+=one[0][i]*New[i]\n",
        "    for i in range(m):\n",
        "      New_next[i]+=(1-alpha)*row_sum/m\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Check for convergence\n",
        "    if np.sum(np.abs(New_next - New)) < tolerance:\n",
        "        break\n",
        "    New = New_next / np.sum(New_next)  # Normalize the PageRank vector\n",
        "    iterations += 1\n",
        "\n",
        "# Print I in order of pageId\n",
        "total_sum = np.sum(New)\n",
        "for i in range(len(New)):\n",
        "    print(pages[i], '=', New[i][0])\n",
        "\n",
        "print('s =', total_sum)"
      ]
    }
  ]
}